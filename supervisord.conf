[supervisord]
nodaemon=true
logfile=/var/log/supervisor/supervisord.log
pidfile=/var/run/supervisord.pid
childlogdir=/var/log/supervisor
loglevel=info
user=root

[program:vllm]
command=python3 -m vllm.entrypoints.openai.api_server --model %(ENV_VLLM_MODEL)s --host %(ENV_VLLM_HOST)s --port %(ENV_VLLM_PORT)s --gpu-memory-utilization %(ENV_VLLM_GPU_MEMORY_UTILIZATION)s --max-model-len %(ENV_VLLM_MAX_MODEL_LEN)s --tensor-parallel-size %(ENV_VLLM_TENSOR_PARALLEL_SIZE)s --trust-remote-code --dtype auto --enable-auto-tool-choice --tool-call-parser hermes
directory=/app
autostart=true
autorestart=true
startretries=3
redirect_stderr=true
stdout_logfile=/var/log/supervisor/vllm.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/vllm_error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
priority=100
stopwaitsecs=30
killasgroup=true
stopasgroup=true

[program:fastapi]
command=python3 -m uvicorn server:app --host %(ENV_API_HOST)s --port %(ENV_API_PORT)s --log-level info --no-access-log
directory=/app/api
autostart=true
autorestart=true
startretries=3
redirect_stderr=true
stdout_logfile=/var/log/supervisor/fastapi.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/fastapi_error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
priority=200
startsecs=10
stopwaitsecs=10
killasgroup=true
stopasgroup=true
# Wait for vLLM to be ready before starting
depends_on=vllm

[program:vllm-healthcheck]
command=/bin/bash -c "sleep 30 && while true; do curl -f http://localhost:%(ENV_VLLM_PORT)s/health > /dev/null 2>&1 || exit 1; sleep 30; done"
autostart=true
autorestart=true
redirect_stderr=true
stdout_logfile=/dev/null
stderr_logfile=/dev/null
priority=300
startsecs=0

[eventlistener:processes]
command=/app/scripts/supervisor_healthcheck.py
events=PROCESS_STATE,TICK_60
autostart=true
autorestart=true
redirect_stderr=true
stdout_logfile=/var/log/supervisor/healthcheck.log
stderr_logfile=/var/log/supervisor/healthcheck_error.log

