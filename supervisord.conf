[supervisord]
nodaemon=true
logfile=/var/log/supervisor/supervisord.log
pidfile=/var/run/supervisord.pid
childlogdir=/var/log/supervisor
loglevel=info
user=root

[program:vllm]
environment=VLLM_MODEL="%(ENV_VLLM_MODEL)s",VLLM_PORT="%(ENV_VLLM_PORT)s",VLLM_HOST="%(ENV_VLLM_HOST)s",VLLM_GPU_MEMORY_UTILIZATION="%(ENV_VLLM_GPU_MEMORY_UTILIZATION)s",VLLM_MAX_MODEL_LEN="%(ENV_VLLM_MAX_MODEL_LEN)s",VLLM_TENSOR_PARALLEL_SIZE="%(ENV_VLLM_TENSOR_PARALLEL_SIZE)s",VLLM_TRUST_REMOTE_CODE="%(ENV_VLLM_TRUST_REMOTE_CODE)s"
command=python3 -m vllm.entrypoints.openai.api_server --model %(ENV_VLLM_MODEL)s --host %(ENV_VLLM_HOST)s --port %(ENV_VLLM_PORT)s --gpu-memory-utilization %(ENV_VLLM_GPU_MEMORY_UTILIZATION)s --max-model-len %(ENV_VLLM_MAX_MODEL_LEN)s --tensor-parallel-size %(ENV_VLLM_TENSOR_PARALLEL_SIZE)s --trust-remote-code --dtype auto
directory=/app
autostart=true
autorestart=true
startretries=3
redirect_stderr=true
stdout_logfile=/var/log/supervisor/vllm.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/vllm_error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
priority=100
stopwaitsecs=30
killasgroup=true
stopasgroup=true

[program:fastapi]
environment=API_HOST="%(ENV_API_HOST)s",API_PORT="%(ENV_API_PORT)s",VLLM_BASE_URL="%(ENV_VLLM_BASE_URL)s",TTS_DEVICE="%(ENV_TTS_DEVICE)s",HF_TOKEN="%(ENV_HF_TOKEN)s"
command=python3 -m uvicorn server:app --host %(ENV_API_HOST)s --port %(ENV_API_PORT)s --log-level info --no-access-log
directory=/app/api
autostart=true
autorestart=true
startretries=3
redirect_stderr=true
stdout_logfile=/var/log/supervisor/fastapi.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/fastapi_error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
priority=200
startsecs=10
stopwaitsecs=10
killasgroup=true
stopasgroup=true
# Wait for vLLM to be ready before starting
depends_on=vllm
